# Doing this as YAML just because that means I can put comments and newlines and stuff
name: "Abide and recon script"

# Things like environment variables that all scripts need Nothing
# computationally expensive, but likely where parameters and such would go
# Please note, no #!/bin/bash -- this is added by the backend!
header: |
  export alri=/grps2/alri
  export scratch=/scratch/$USER
  export abide=$scratch/abide

# Here goes all the code needed to setup the job this is basically
# abide_organizer.sh, minus the vars in header and the #!/bin/bash all the setup
# needed for the job, run on compute (like here it's extracting the data and
# copying things)
setupScript: |
  # Create a scratch directory for user if it doesn't exist
  # If loop starts with conditions in brackets, followed by 'then' and ending with 'fi'
  if [ ! -d $abide ]; then
      echo ... Creating $abide
      mkdir -p $abide;
  fi

  echo ... Copying abide data to scratch directory
  cp -r $alri/abide $scratch
  cd $abide

  ## === This is the section where you must be logged in to compute node ===

  echo ... Extracting ABIDE data
  tar -xvzf NYU_a.tgz
  tar -xvzf NYU_d.tgz

  # === Create a directory for segmentation. This will be used for Freesurfer data ===
  segdir=$abide/segmentation
  mkdir -p $segdir

  # Copy all the mprage.nii.gz files from NYU and name them subjid.nii.gz
  cd $abide/NYU

  ## === This loop searches all the mprage.nii.gz files from NYU directory
  ## === Locates the subjectID by cutting the file path into separate fields
  ## === Creates name for output file as subjectID.nii.gz using subjectID from previous line
  ## === Copies the mprage from NYU to the segmentation folder with the name subjectID.nii.gz

  ## for loops starts with 'do' and ends with 'done' and runs on the list
  ## specified in variable (struct in our case)

  for struct in `find . -name 'mprage.nii.gz'`; do
      subject=`echo $struct | cut -d '/' -f2`;
      outfile=`printf "%s.nii.gz" $subject`;
      echo ... Copying $struct to $segdir/$outfile
      cp $struct $segdir/$outfile;
  done

# This was not provided, in his examples he just used `srun < the-file.sh`. This
# is normally fine, but adding a slurm config allows more control. Not all of
# these are necessary (not sure any of them are)?  However, the QOS at least is
# good to have, in case a setup job may take a certain amount of time or
# something
setupScriptSlurmConfig: |
  #SBATCH -J test # job name
  #SBATCH -n 1 # number of tasks to use (usually 1)
  #SBATCH -c 1 # number of threads you are going to use
  #SBATCH -p main # main partition
  #SBATCH --qos debug # queue, 15min limit

# This is everything above the loop, including the top of the main loop.  This
# can include some extra (very minor) setup stuff but mainly is just the for
# loop which will determine what jobs we have
loopWrapperTop: |
  #!/bin/bash

  subjectsDirectory=/scratch/$USER/abide/segmentation

  cd $subjectsDirectory
  if [ -d jobs ]; then
      rm -Rf jobs
  fi

  if [ ! -d jobs ]; then
      mkdir jobs
  fi

  # The important bit!!
  for subs in `ls [005]*.nii.gz | sort | tail -10`
  do
    # this is inside the loop, but before the part being used to generate the job text
    # since the job script required $subject, this goes here
    subject=`echo $subs | cut -d '.' -f1`;

# From the for loop, what's the index variable that uniquely identifies each job?
idVariable: $subs

# Now, we grab the part of the script containing the actual job
# this can contain any variables from the script/environment, like $subject or $USER
jobTemplate: |
  #!/bin/bash

  module load bio/freesurfer/7.1.1;
  source /share/apps/freesurfer_7_1_1/freesurfer/SetUpFreeSurfer.sh

  export SUBJECTS_DIR=/scratch/$USER/abide/segmentation;
  export FS_LICENSE=/grps2/alri/workshop/.license.txt
  recon-all -i ${subject}.nii.gz -subjid ${subject} -all -qcache

# The directives for submitting the jobTemplate to slurm.  Please note, this
# should NOT include the srun -- the backend's generation will add this in.  It
# should only be the config headers
slurmTemplate: |
  #!/bin/bash

  #SBATCH --job-name=$subject
  #SBATCH --output=res_$subject.txt
  #SBATCH --ntasks=1
  #SBATCH --cpus-per-task=2
  #SBATCH --partition=main
  #SBATCH --mem-per-cpu=8192
  #
  # Time format = HH:MM:SS, DD-HH:MM:SS
  #
  #SBATCH --time=20:00:00

# We're omitting the chmod and sbatch calls since those will be done by the
# backend.  All we need is to close out the for loop started in loopWrapperTop;
# since there was only one level of looping then only one `done` is needed:
loopWrapperBottom: |
  done

# We didn't get an example of this, but it would be the opposite of the setup
# script.  It'd delete the temp files and copy the results back into the shared
# drive space
cleanupScript: "n/a"
# same as the other slurm configs, just includes configuration variables
cleanupSlurmConfig: "n/a"

# we don't have a cleanup script, so we don't ever want to run one.
# other values could be ALL_ENDED, to run after all jobs ended, for example
cleanupMode: NEVER

# If this script should be visible to all users (should anyone be able to create
# global templates? or just admins?). Since this is an example one Deshpande gave
# us, I'm going to go with yes it should be globally accessible.
globalTemplate: true
